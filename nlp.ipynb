{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which is better for news article classification: Naive Bayes using Count Vectorizer or Logistic Regression using word vectors from a model trained on Google News?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google has pretrained a neural network on news articles. We can extract the word vectors, or word embeddings, that the model has learned and use them to train our own model. Let's load in the model here. It is relatively large so it will take a minute to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    './GoogleNews-vectors-negative300.bin', \n",
    "    binary=True\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to load in the text from the news articles that we want to categorize. If we use Scikit Learn's `load_files` function from `sklearn.datasets`, the data and categories will be automatically loaded, so long as the text files are in properly labeled folders.\n",
    "\n",
    "The `news` variable will store both data and target labels, each of which can be accessed by calling `news.data` and `news.target`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = load_files(\n",
    "    './bbc/', \n",
    "    encoding='utf-8',\n",
    "    decode_error='ignore',\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display a few sentences of one of the articles to get an idea of what they look like, and see what the target labels look like as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_n_sentences(article:str, n):\n",
    "    \"Returns the first n sentences in an article, or None if that number of sentences does not exist\"\n",
    "    first_n = ''\n",
    "    period_counter = 0\n",
    "    for character in article:\n",
    "        first_n += character\n",
    "        if character == '.':\n",
    "            period_counter += 1\n",
    "        if period_counter == n:\n",
    "            return first_n\n",
    "    print(f'There are not {n} sentences in this article')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UK house prices dip in November\\n\\nUK house prices dipped slightly in November, the Office of the Deputy Prime Minister (ODPM) has said.\\n\\nThe average house price fell marginally to £180,226, from £180,444 in October. Recent evidence has suggested that the UK housing market is slowing after interest rate increases, and economists forecast a drop in prices during 2005.'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_n_sentences(news.data[0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Collins named UK Athletics chief\\n\\nUK Athletics has ended its search for a new performance director by appointing psychologist Dave Collins.\\n\\nCollins, who worked with the British teams at the 2000 and 2004 Olympics, takes over from Max Jones. Six candidates were interviewed for the job, including Denise Lewis' coach Charles van Commenee and former British triple jumper Keith Connor.\""
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_n_sentences(news.data[1000], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['business', 'entertainment', 'politics', 'sport', 'tech']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes can take counts of words and generate a model that looks at condtitional class probabilities. In this case, the classes correspond to the five categories of news. Let's start with Naive Bayes since it is quick to prepare. We can use Scikit Learn's `Pipeline` class to easily pass the data through the counting and fitting steps of the Naive Bayes modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_classification(clf: Pipeline, train_data, test_data, train_target, test_target):\n",
    "    \"Helper function wrapping the Pipeline, courtesy of Brian Spiering\"\n",
    "    clf.fit(train_data, train_target) \n",
    "    predicted = clf.predict(test_data)\n",
    "    accuracy = np.mean(predicted==test_target)\n",
    "    print(f\"The accuracy on the test data is {accuracy:.2%}\")\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([('vect', CountVectorizer()),\n",
    "                ('clf', MultinomialNB())])\n",
    "nb_scores = cross_val_score(clf, news.data, news.target, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (nb_scores.mean(), nb_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is already doing a great job, predicting close to 98%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize and remove punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is helpful to make things easy for the model, which is why we will clean and tokenize the text. The `Spacy` library does a great job of tokenizing, or separating out individual words. This process counts punctuation characters and numbers as tokens, so we can follow up with a very basic regular expression substitution that removes these. Removing punctuation may not always be useful, like in the case of sentiment analysis where punctuation may add additional meaning or emphasis. In our case, punctuation will likely not give any clues to the topic of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def tokenize_bbc(text_data):\n",
    "    \"Tokenize BBC news articles and remove punctuation and digits\"\n",
    "    clean_text = list()\n",
    "    for text_chunk in text_data:\n",
    "        doc = nlp(text_chunk)\n",
    "        clean = ' '.join([token.text for token in doc])\n",
    "        clean = re.sub('[\\\\n\\.\\,\\(\\)\\'\\\"\\-\\:\\;\\&\\#\\/0-9]', '', clean)\n",
    "        clean_text.append(clean)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.data = tokenize_bbc(news.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the unique words in the corpus and see how many match up with the model vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to use pre-trained word vectors to train a classification model. If none of the words in our articles match up with words in the pre-trained model, then this effort will be for nothing. This next step checks to make sure we are capturing enough of the words to have good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_words(text:list): \n",
    "    word_set = set()\n",
    "    for word_list in text:\n",
    "        word_set = word_set | set(word_list)\n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matched_vocab(articles:list, model:gensim):\n",
    "    data_vocab = find_unique_words([article.split() for article in articles])  # Vocabulary of the dataset\n",
    "    model_vocab = set(model.vocab.keys())  # Vocabulary of the Google model\n",
    "    matched_vocab = data_vocab & model_vocab  # Intersecting vocabulary\n",
    "    print(f'{len(matched_vocab) * 100 / len(data_vocab) : .2f}% of the dataset vocabulary has been matched')\n",
    "    return matched_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97.00% of the dataset vocabulary has been matched\n"
     ]
    }
   ],
   "source": [
    "matched_vocab = calculate_matched_vocab(news.data, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert words to embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we create a dictionary that allows us to look up the word embedding for any word. We choose to only include matched words, since those are the only ones that will be used. Then we convert each word in each article to its corresponding word vector. Finally we save our converted embedded text so we do not have to repeat these intial preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dict = {word:model.get_vector(word) for word in matched_vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_vector(data, embed_dict, matched_vocab):\n",
    "    \"Converts string tokens to word vectors using an embedding dictionary and a set of intersecting vocabulary.\"\n",
    "    embedded_text = list()\n",
    "    for article in data:\n",
    "        embedded_text.append(\n",
    "            np.array([embed_dict[word] for word in article.strip().split() if word in matched_vocab])\n",
    "        )\n",
    "    return embedded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_text = string_to_vector(news.data, embed_dict, matched_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the minimum word article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using regression, our inputs must all be the same length. To ensure this, we can find the article with the smallest number of words and then subset all other articles by that length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink_to_smallest(embedded_text):\n",
    "    \"Shrinks all embedded text documents to match the minimum length document\"\n",
    "    min_length = np.min([article.shape[0] for article in embedded_text])\n",
    "    embedded_text = [article[:min_length] for article in embedded_text]\n",
    "    return embedded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_text = shrink_to_smallest(embedded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save our work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are done pre-processing, we should save our work. We don't want to have to repeat all these steps if we just want to tune or re-run the model in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./tmp/bbc_text_embed_converted.npy', embedded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_text = np.load('./tmp/bbc_text_embed_converted.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can average the word embeddings to give a notion of meaning to each article and then fit a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_average_vectors(embedded_text):\n",
    "    \"Calculate the average of the word embeddings for each text in a corpus\"\n",
    "    avg_embed_text = list()\n",
    "    for article in embedded_text:\n",
    "        avg_embed_text.append(np.sum([word for word in article], axis=0) / len(article))\n",
    "    return np.array(avg_embed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.98 (+/-  0.02)\n"
     ]
    }
   ],
   "source": [
    "avg_embed_text = create_average_vectors(embedded_text)\n",
    "clf = lr = LogisticRegression(C=100)\n",
    "lr_scores = cross_val_score(clf, avg_embed_text, news.target, cv=10)\n",
    "print(f'Accuracy: {lr_scores.mean() : 0.2f} (+/- {lr_scores.std() * 2 : 0.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat with newsgroup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './newsgroups/'\n",
    "boards = load_files(\n",
    "    data_path, \n",
    "    encoding='utf-8',\n",
    "    decode_error='ignore',\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_newsgroup(text_data):\n",
    "    \"Tokenize newsgroup posts\"\n",
    "    clean_text = list()\n",
    "    for text_chunk in text_data:\n",
    "        text_chunk = re.sub('[\\\\n\\.\\,\\(\\)\\'\\\"\\-\\:\\;\\&\\#\\!\\*\\<\\>\\@\\^\\`\\~\\|\\\\\\$\\/0-9]', ' ', text_chunk)\n",
    "        doc = nlp(text_chunk)\n",
    "        clean = ' '.join([token.text.strip() for token in doc if len(token) > 1])\n",
    "        clean_text.append(clean)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "boards.data = tokenize_newsgroup(boards.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60.31% of the dataset vocabulary has been matched\n"
     ]
    }
   ],
   "source": [
    "matched_vocab_2 = calculate_matched_vocab(boards.data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dict_2 = {word:model.get_vector(word) for word in matched_vocab_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_text_2 = string_to_vector(boards.data, embed_dict_2, matched_vocab_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_text_2 = shrink_to_smallest(embedded_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./tmp/newsgroup_text_embed_converted.npy', embedded_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_text_2 = np.load('./tmp/newsgroup_text_embed_converted.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.98 (+/-  0.01)\n"
     ]
    }
   ],
   "source": [
    "avg_embed_text_2 = create_average_vectors(embedded_text_2)\n",
    "clf_2 = LogisticRegression(C=100)\n",
    "lr_scores_2 = cross_val_score(clf_2, avg_embed_text_2, boards.target, cv=10)\n",
    "print(f'Accuracy: {lr_scores_2.mean() : 0.2f} (+/- {lr_scores_2.std() * 2 : 0.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
